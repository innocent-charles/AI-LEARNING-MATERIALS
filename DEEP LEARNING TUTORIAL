LIBRARY FOR DEEP LEARNING 
-Tensorflow(Google)
 -Keras
 

-Cntk(Microsoft)
 -Keras 
 
 
-Theano 
 -Keras
 
 
-Pytorch (Facebook)


DEEP LEARNING  ALGORITHMS IN MOSTLY !!! 
-ANN
 -Regression 
 -Classification
 
-CNN 
 -Images
 -Audio
 -Videos
 

-RNN 
 -Text(NLP PROJECTS)
 
-LSTM 

-GANNs(Generative Adversial Neuron Network)
 
 -AutoIncoders
 
 -Feed forward Neural Networks
 
 
DEEP LEARNING TUTORIAL 
Dfn:-This is the subset of the machine learning that used the neural networks so as mimic the function of the  brain .,it actual mean machine learns a concecutive layers of presentation.
-MIMIC -means to make the machine think and act as a human brain.

NEURAL NETWORKS 
Dfn:is the network(system) of neurons either artificial or organic(biological).
-Therefore in deep learning since we want to mimic the brain's functionality hence we have ARTIFICIAL NEURAL  NETWORKS.


-This neural networks takes the numerical inputs and then combines with bias and group of weights then produces a single outputs 
-Nerual networks  has many perceptron  layers hence it is called multi-layer perceptrons !! 


PERCEPTRON -Dfn:-is the simple and single neural network.This has only one hidden layer.



NEURONS in Human:
-Dedrites
-Axon
-synapse
-neclue


NEURAL Network in Deep Learning:
-Input signals
-Output Signals
-Weight
-Bias 


NERUAL LAYERS IN DEEP LEARNING
-Input Layers -this actual takes the inputs signal or independent neurons 
-Hidden Layers -This is where the actual work is done about classifying or predicting 
-Output Layers  -This takes the outputs


































ACTIVATION FUNCTION
Elements of the Nerual Networks :
-Input layer
-Hidden layer
-Output layer




Dfn:This is the function that calculates the average weighted sum of the input and it adds the bias ,then decides wheather the neuron should be activated or what !!!! 
-The purpose of the activation function is to introduce the non-linearity into output of the neuron.


Type of the Activation Functions;
-Linear Function
 This used to deal with the problems that the output has to be a linear function.
-Sigmoid Function 
 This used to deal with the problems that the output has to be a binary classification 
-Tanh Function 
-ReLU Function 
 This is mostly used in the all scenerio 
-Softmax Function 
 This is used when the output has to be a multi class classification 
 

COST FUNCTION 
Dfn:-This is the function that measures the performance of the machine learning model by the given number of data.
Example:
-Quadratic function 
-Enthropy Function 


GRADIENT DESCENT 
-This is the optimization algorithm that used to minimizing the cost function.
 This occurs simply by taking the gradient(slope) or doing derivates of the certain function so as to reduce the cost of fucntion . 
 
 
BACKPROPAGATION 
This is the technique that uses  the gradient descent to find out the best parameter for minimizing the cost function 



HOW NEURAL NETWORKS WORKS ?
-1.First the neural network takes  the inputs in form of the numerical. 
 Example:let's say x1,x2,x3.
-2.Then the two processing happens in the neurons(Hidden layers)
 -Process One:
 >it performs the summation of the product of the weights and inputs  then add bias 
 Example:y=x1w1+x2w2+x3w3
 >It apply the activation function to the resulted function of the sum .
 Example:z=activation(y)
 >Then from the hidden layer to the output layer the resulted function from step 2 that is z is multiplied to it's  weghts
 Eaxmple.p=zw4 
3.Then the activation function is applied to the neurons to the function p.,
 then it produces the outputs.

Note from the above that is what is called the  FORWARD PROPAGATION.


-From one layer to other layer ,the data entered in form of the matrix.
Example: 4x3 matrix  means the from the layers with four features(input) to the layer with the three features(inputs) and go on .


ACTIVATIONS FUNCTION-is the function that calculates the average weights of the inputs signal then add bias to it ,then check whether the neuron should be activated or not .
Eaxample:-sigmoid
         -linear
         -tanh
         -relu
         -softmax
         -Elu
         -Prelu
         -swish
         -softplus 
         -leaky relu
         
         
Sigmoid function:
-This transform the results in neurons between 0 and 1 ,z=(1)/(1+e-y)
Relu
-This provides the non linearlity to the neorons and also resulted the maximum,max(y,0)





FORWARD PROPAGATION AND BACKWARD PROPAGATION(How the train happens in neural network)
LOSS FUNCTION(COST FUNCTION)-is the squared of the  difference between the actual value to the predicted values.loss=(ya-yp)^2.
-The loss function is used to check the performance of the model created,hence it has to be low as possible simply by properly putting the values of the weights(updating the weights) that is done by the optimizer by using backpropagation(optimization function Eaxmple Gradient descent or stotastichic gradient descent).



FORWARD PROPAGATION-it the proprocess or technique of finding the outputs as the inputs fed in the neural network where by each neurons accepts the inputs process by adding the the products of the weights and the inputs  and add bias then apply the activation function then passes it to the succesive neurons.



BACKPROPAGATION -This is the technique or process that uses the optimizer such as gradient descent or the statichiostric gradient descent to reduce the loss function or cost function simply by going back from the output layer to the inputs so as to updates the weights.
-This actually occurs like this: weightnew=weightoriginal-(learning rate * (derivative of the loss function with respect to the weightoriginal)


NOW TO TRAIN MULTI-NEURAL NETWORK
Dfn:MUlti-layer neural networks is the neural network that has the more than one hidden layer.
-From one layer to other layer ,the data entered in form of the matrix.
Example: 4x3 matrix  means the from the layers with four features(input) to the layer with the three features(inputs) and go on.


Note:When training ,The ONE EPOCH is the simply when the model peforms the forward and back propagation respectively.


VANISHING GRADIENT PROBLEM 
-This is problem that caused by the sigmoid function and also the tanh function  when doing backpropagation by the gradient descent to find out the updated weight so as to reduces the cost/loss function.Where by the updated weight and the original weight were found to be nearly the same because the range of the derivatives is range from 0-0.25.hence that's why the sigmoid activation function is not used in the hidden layers.



-Therefore to solve the problem of the vanishing gradient problem .we use the activation funcion of the relu where by the range of the derivatives is either 0 or 1,actually it used leaky leru whereby to avoid the dead neuron to occur because of the value to be 0  hence there will be a maximum difference between the original and updated weight.


EXPLODING GRADIENT PROBLEM 
-This is the problem that caused by the weights 


DROP OUT AND REGULARIZATION 
-This is used to reduce or solve the problem of the overfiting 
Note that:Overfiting in neural networks happen because of the using the single neural network or using to many more neural networks.



DROP OUT:
-This was discovered in 2014 by the two people that are the Jeoffrey Hinton and Nitish.
-Dfn:-This refer to the paramater that used to reduce the overfiting of the model by using the certain drop out ratio from the subset features by deactivating them.
-This always range from the 0-1.
-This value of the drop out ratio is selected or found by using the hyperparameter optimazition or cross validation.
-But the general concept is it has to be high even greater than 0.5 




VARIOUS WEIGHT INITIALIZATION TECHNIQUE 
key points:
-Weight should be medium small 
-Weight should not be same
-Weight should have a good variance.


Techniques used:
Uniform Distribution 
 :That is the weights will be uniform distributed such that between a and b where a=-[1/square root(inputs)] and b=[1/square root(inputs)] .
-This works very nice with the sigmoid function 


Xavier and Gorat techniques 
-This works with the sigmod.

He init
-This works very nice with the relu activation function.



OPTIMIZERS IN DEEP LEARNING 
-mini batch SGD -This is used to takes the batch size of the records or data set to updates the weights. 


-GD -This takes the all records in  all dataset to updates the weights 

-SGD -This takes only one record to updates the weights 

-adagrad(adaptive GD) -this is used to train the model by using different learning rate in the every iterations. 

-Adams  -The every one uses the adamas 

-adadelta 



TWO TYPES OF THE MODELS FROM SCRATCH USED IN DEEP LEARNING
- Sequential API ,This is very simple and recommended for starting , It is referred to as “sequential” because it involves defining a Sequential class and adding layers to the model one by one in a linear manner, from input to output.
-Functional API (Advanced)








